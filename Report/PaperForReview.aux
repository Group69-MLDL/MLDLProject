\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{1_Ego4D_Around_the_World_in_3000_Hours_of_Egocentric_Video}
\citation{4_Span_based_Localizing_Network_for_Natural_Language_Video_Localization}
\citation{8_OMNIVORE_ASingle_Model_for_Many_Visual_Modalities}
\citation{6_Egocentric_Video_Language_Pretraining}
\citation{11_Liu2024CogVLM2_Visual_Language_Models_for_Image_and_Video_Understanding}
\citation{12_Liu2024LLaVANeXT_Tackling_Multi-image_Video_and_3D_in_Large_Multimodal_Models}
\citation{13_Wang2024InternVideo_InternVideo2.5_Empowering_Video_MLLMs_with_Long_and_Rich_Context_Modeling}
\citation{1_Ego4D_Around_the_World_in_3000_Hours_of_Egocentric_Video}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{brf}{\backcite{1_Ego4D_Around_the_World_in_3000_Hours_of_Egocentric_Video}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{4_Span_based_Localizing_Network_for_Natural_Language_Video_Localization}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{8_OMNIVORE_ASingle_Model_for_Many_Visual_Modalities}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{6_Egocentric_Video_Language_Pretraining}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{11_Liu2024CogVLM2_Visual_Language_Models_for_Image_and_Video_Understanding}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{12_Liu2024LLaVANeXT_Tackling_Multi-image_Video_and_3D_in_Large_Multimodal_Models}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{13_Wang2024InternVideo_InternVideo2.5_Empowering_Video_MLLMs_with_Long_and_Rich_Context_Modeling}{{1}{1}{section.1}}}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Related Work}{1}{section.2}\protected@file@percent }
\newlabel{sec:relatedWork}{{2}{1}{\hskip -1em.~Related Work}{section.2}{}}
\newlabel{sec:relatedWork@cref}{{[section][2][]2}{[1][1][]1}}
\citation{4_Span_based_Localizing_Network_for_Natural_Language_Video_Localization}
\citation{3_TALL_Temporal_Activity_Localization_via_Language_Query}
\citation{8_OMNIVORE_ASingle_Model_for_Many_Visual_Modalities}
\citation{6_Egocentric_Video_Language_Pretraining}
\citation{12_Liu2024LLaVANeXT_Tackling_Multi-image_Video_and_3D_in_Large_Multimodal_Models}
\citation{14_Liu2023LLaVA_Large_Language_and_Vision_Assistant}
\citation{11_Liu2024CogVLM2_Visual_Language_Models_for_Image_and_Video_Understanding}
\citation{13_Wang2024InternVideo_InternVideo2.5_Empowering_Video_MLLMs_with_Long_and_Rich_Context_Modeling}
\citation{1_Ego4D_Around_the_World_in_3000_Hours_of_Egocentric_Video}
\@writefile{brf}{\backcite{1_Ego4D_Around_the_World_in_3000_Hours_of_Egocentric_Video}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{4_Span_based_Localizing_Network_for_Natural_Language_Video_Localization}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{3_TALL_Temporal_Activity_Localization_via_Language_Query}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{8_OMNIVORE_ASingle_Model_for_Many_Visual_Modalities}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{6_Egocentric_Video_Language_Pretraining}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{12_Liu2024LLaVANeXT_Tackling_Multi-image_Video_and_3D_in_Large_Multimodal_Models}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{14_Liu2023LLaVA_Large_Language_and_Vision_Assistant}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{11_Liu2024CogVLM2_Visual_Language_Models_for_Image_and_Video_Understanding}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{13_Wang2024InternVideo_InternVideo2.5_Empowering_Video_MLLMs_with_Long_and_Rich_Context_Modeling}{{2}{2}{section.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}Methodology}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em.\nobreakspace  {}Datasets}{2}{subsection.3.1}\protected@file@percent }
\@writefile{brf}{\backcite{1_Ego4D_Around_the_World_in_3000_Hours_of_Egocentric_Video}{{2}{3.1}{subsection.3.1}}}
\citation{6_Egocentric_Video_Language_Pretraining}
\citation{8_OMNIVORE_ASingle_Model_for_Many_Visual_Modalities}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Pipeline from query-segment prediction to annotation of trimmed clips for VLM input.\relax }}{3}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:FullDiagram}{{1}{3}{Pipeline from query-segment prediction to annotation of trimmed clips for VLM input.\relax }{figure.caption.1}{}}
\newlabel{fig:FullDiagram@cref}{{[figure][1][]1}{[1][2][]3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Distribution of clip sizes (in seconds) for NLQ training data.\relax }}{3}{figure.caption.2}\protected@file@percent }
\newlabel{fig:clip_dist}{{2}{3}{Distribution of clip sizes (in seconds) for NLQ training data.\relax }{figure.caption.2}{}}
\newlabel{fig:clip_dist@cref}{{[figure][2][]2}{[1][3][]3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Distribution of queries where the segment size is $> 20\%$ of the clip.\relax }}{3}{figure.caption.3}\protected@file@percent }
\newlabel{fig:query_gt_0_2}{{3}{3}{Distribution of queries where the segment size is $> 20\%$ of the clip.\relax }{figure.caption.3}{}}
\newlabel{fig:query_gt_0_2@cref}{{[figure][3][]3}{[1][3][]3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Distribution of queries where the segment size is $ \leqslant 20\%$ of the clip.\relax }}{3}{figure.caption.4}\protected@file@percent }
\newlabel{fig:query_lte_0_2}{{4}{3}{Distribution of queries where the segment size is $ \leqslant 20\%$ of the clip.\relax }{figure.caption.4}{}}
\newlabel{fig:query_lte_0_2@cref}{{[figure][4][]4}{[1][3][]3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\hskip -1em.\nobreakspace  {}Pre-Extracted Features}{3}{subsection.3.2}\protected@file@percent }
\@writefile{brf}{\backcite{6_Egocentric_Video_Language_Pretraining}{{3}{3.2}{subsection.3.2}}}
\citation{4_Span_based_Localizing_Network_for_Natural_Language_Video_Localization}
\citation{4_Span_based_Localizing_Network_for_Natural_Language_Video_Localization}
\citation{4_Span_based_Localizing_Network_for_Natural_Language_Video_Localization}
\@writefile{brf}{\backcite{8_OMNIVORE_ASingle_Model_for_Many_Visual_Modalities}{{4}{3.2}{subsection.3.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}\hskip -1em.\nobreakspace  {}Temporal Localization Models}{4}{subsection.3.3}\protected@file@percent }
\newlabel{sec:temporal}{{3.3}{4}{\hskip -1em.~Temporal Localization Models}{subsection.3.3}{}}
\newlabel{sec:temporal@cref}{{[subsection][3][3]3.3}{[1][4][]4}}
\@writefile{brf}{\backcite{4_Span_based_Localizing_Network_for_Natural_Language_Video_Localization}{{4}{3.3}{subsection.3.3}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}\hskip -1em.\nobreakspace  {}Architectural Variants of VSLNet}{4}{subsection.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The VSLNet architecture, highlighting the dynamic span prediction over video segments \cite  {4_Span_based_Localizing_Network_for_Natural_Language_Video_Localization}.\relax }}{4}{figure.caption.5}\protected@file@percent }
\@writefile{brf}{\backcite{4_Span_based_Localizing_Network_for_Natural_Language_Video_Localization}{{4}{5}{figure.caption.5}}}
\newlabel{fig:VSLNet_VSLBase}{{5}{4}{The VSLNet architecture, highlighting the dynamic span prediction over video segments \cite {4_Span_based_Localizing_Network_for_Natural_Language_Video_Localization}.\relax }{figure.caption.5}{}}
\newlabel{fig:VSLNet_VSLBase@cref}{{[figure][5][]5}{[1][4][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}\hskip -1em.\nobreakspace  {}From Temporal Localization to Answer Generation}{4}{subsection.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}\hskip -1em.\nobreakspace  {}Answer Generation with VideoQA Models}{5}{subsection.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}Experiments and results}{5}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}\hskip -1em.\nobreakspace  {}Training Parameters}{5}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}\hskip -1em.\nobreakspace  {}Intersection over Union}{5}{subsection.4.2}\protected@file@percent }
\newlabel{eq:iou_formula}{{1}{5}{\hskip -1em.~Intersection over Union}{equation.4.1}{}}
\newlabel{eq:iou_formula@cref}{{[equation][1][]1}{[1][5][]5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}\hskip -1em.\nobreakspace  {}Temporal Localization Performance}{5}{subsection.4.3}\protected@file@percent }
\citation{5_SlowFast_Networks_for_Video_Recognition}
\citation{1_Ego4D_Around_the_World_in_3000_Hours_of_Egocentric_Video}
\citation{1_Ego4D_Around_the_World_in_3000_Hours_of_Egocentric_Video}
\@writefile{brf}{\backcite{5_SlowFast_Networks_for_Video_Recognition}{{6}{\caption@xref {??}{ on input line 290}}{table.caption.6}}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison of VSLNet and VSLBase Performance Using Omnivore and EgoVLP Features Against Official Ego4D Baseline \cite  {1_Ego4D_Around_the_World_in_3000_Hours_of_Egocentric_Video}. The results of VSLBase on EgoVLP dataset proves better performance among all the combinations.\relax }}{6}{table.caption.6}\protected@file@percent }
\@writefile{brf}{\backcite{1_Ego4D_Around_the_World_in_3000_Hours_of_Egocentric_Video}{{6}{1}{table.caption.6}}}
\newlabel{tab:net_base}{{1}{6}{Comparison of VSLNet and VSLBase Performance Using Omnivore and EgoVLP Features Against Official Ego4D Baseline \cite {1_Ego4D_Around_the_World_in_3000_Hours_of_Egocentric_Video}. The results of VSLBase on EgoVLP dataset proves better performance among all the combinations.\relax }{table.caption.6}{}}
\newlabel{tab:net_base@cref}{{[table][1][]1}{[1][6][]6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}\hskip -1em.\nobreakspace  {}Evaluation of VSLNet Variants}{6}{subsection.4.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Evaluation of VSLNet variants with different encoder configurations—GloVe and Non-Shared Encoders—across Omnivore and EgoVLP datasets. The results of NSE on EgoVLP dataset demonstrate higher scores compared to other experiments.\relax }}{6}{table.caption.7}\protected@file@percent }
\newlabel{tab:Glove_NSE}{{2}{6}{Evaluation of VSLNet variants with different encoder configurations—GloVe and Non-Shared Encoders—across Omnivore and EgoVLP datasets. The results of NSE on EgoVLP dataset demonstrate higher scores compared to other experiments.\relax }{table.caption.7}{}}
\newlabel{tab:Glove_NSE@cref}{{[table][2][]2}{[1][6][]6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}\hskip -1em.\nobreakspace  {}NLP Metrics for Answer Generation}{6}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}\hskip -1em.\nobreakspace  {}Generative Answering Results}{7}{subsection.4.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces BLEU and ROUGE scores of evaluated VLMs.\relax }}{7}{table.caption.8}\protected@file@percent }
\newlabel{tab:vlms_performance}{{3}{7}{BLEU and ROUGE scores of evaluated VLMs.\relax }{table.caption.8}{}}
\newlabel{tab:vlms_performance@cref}{{[table][3][]3}{[1][7][]7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}\hskip -1em.\nobreakspace  {}Qualitative Examples}{7}{subsection.4.7}\protected@file@percent }
\bibstyle{ieee_fullname}
\bibdata{egbib}
\bibcite{5_SlowFast_Networks_for_Video_Recognition}{1}
\bibcite{3_TALL_Temporal_Activity_Localization_via_Language_Query}{2}
\bibcite{8_OMNIVORE_ASingle_Model_for_Many_Visual_Modalities}{3}
\bibcite{1_Ego4D_Around_the_World_in_3000_Hours_of_Egocentric_Video}{4}
\bibcite{6_Egocentric_Video_Language_Pretraining}{5}
\bibcite{14_Liu2023LLaVA_Large_Language_and_Vision_Assistant}{6}
\bibcite{12_Liu2024LLaVANeXT_Tackling_Multi-image_Video_and_3D_in_Large_Multimodal_Models}{7}
\bibcite{11_Liu2024CogVLM2_Visual_Language_Models_for_Image_and_Video_Understanding}{8}
\bibcite{13_Wang2024InternVideo_InternVideo2.5_Empowering_Video_MLLMs_with_Long_and_Rich_Context_Modeling}{9}
\bibcite{4_Span_based_Localizing_Network_for_Natural_Language_Video_Localization}{10}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Qualitative comparison of answers generated by LLaVA-NeXT, CogVLM2, and InternVideo2.5 for a representative egocentric clip from the Ego4D dataset.\relax }}{8}{figure.caption.9}\protected@file@percent }
\newlabel{fig:VLMs_Qualitative}{{6}{8}{Qualitative comparison of answers generated by LLaVA-NeXT, CogVLM2, and InternVideo2.5 for a representative egocentric clip from the Ego4D dataset.\relax }{figure.caption.9}{}}
\newlabel{fig:VLMs_Qualitative@cref}{{[figure][6][]6}{[1][7][]8}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.\nobreakspace  {}Discussions and findings}{8}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}\hskip -1em.\nobreakspace  {}Acknowledgements}{8}{section.6}\protected@file@percent }
\gdef \@abspage@last{8}
