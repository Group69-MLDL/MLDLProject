@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



%@misc{Authors14,
 %author = {FirstName LastName},
 %title = {The frobnicatable foo filter},
 %note = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
 %year = 2014
%}

@article{9_Video_LLaVA_Learning_United_Visual_Representation_by_Alignment,
	author = {Bin Lin and Yang Ye and Bin Zhu and Jiaxi Cui and Munang Ning and Peng Jin and Li Yuan},
	title = {Video-LLaVA: Learning United Visual Representation by Alignment},
	journal = {arXiv preprint arXiv:2311.10122},
	year = {2024},
	note = {\url{https://github.com/PKU-YuanGroup/Video-LLaVA}}
} % [23]

@article{8_OMNIVORE_ASingle_Model_for_Many_Visual_Modalities,
	author = {Rohit Girdhar and Mannat Singh and Nikhila Ravi and Laurens van der Maaten and Armand Joulin and Ishan Misra},
	title = {OMNIVORE: A Single Model for Many Visual Modalities},
	journal = {arXiv preprint arXiv:2305.06638},
	year = {2023},
	note = {\url{https://facebookresearch.github.io/omnivore}}
} % [24]

@inproceedings{7_Learning_2D_Temporal_Adjacent_Networks_for_Moment_Localization,
	author = {Songyang Zhang and Houwen Peng and Jianlong Fu and Jiebo Luo},
	title = {Learning 2D Temporal Adjacent Networks for Moment Localization with Natural Language},
	booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
	year = {2020}
} % [3]

@inproceedings{4_Span_based_Localizing_Network_for_Natural_Language_Video_Localization,
	author = {Hao Zhang and Aixin Sun and Wei Jing and Joey Tianyi Zhou},
	title = {Span-based Localizing Network for Natural Language Video Localization},
	booktitle = {Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI)},
	year = {2020},
	url = {https://github.com/IsaacChanghau/VSLNet}
} % [30]

@article{6_Egocentric_Video_Language_Pretraining,
	author = {Kevin Qinghong Lin and Alex Jinpeng Wang and Mattia Soldan and Michael Wray and Rui Yan and Eric Zhongcong Xu and Difei Gao and Rongcheng Tu and Wenzhe Zhao and Weijie Kong and Chengfei Cai and Hongfa Wang and Dima Damen and Bernard Ghanem and Wei Liu and Mike Zheng Shou},
	title = {Egocentric Video-Language Pretraining},
	journal = {NeurIPS},
	year = {2022},
	url = {https://github.com/showlab/EgoVLP}
} % [26]

@article{5_SlowFast_Networks_for_Video_Recognition,
	author = {Christoph Feichtenhofer and Haoqi Fan and Jitendra Malik and Kaiming He},
	title = {SlowFast Networks for Video Recognition},
	journal = {arXiv preprint arXiv:1812.03982},
	year = {2019},
	note = {\url{https://github.com/facebookresearch/SlowFast}}
} % [27]

@article{1_Ego4D_Around_the_World_in_3000_Hours_of_Egocentric_Video,
	author = {Kristen Grauman and Andrew Westbury and Eugene Byrne and others},
	title = {Ego4D: Around the World in 3,000 Hours of Egocentric Video},
	journal = {arXiv preprint arXiv:2110.07058},
	year = {2022},
	url = {https://ego4d-data.org/}
} % [28]

@article{2_Rescaling_Egocentric_Vision_Collection_Pipeline_an,
	author = {Dima Damen and Hazel Doughty and Giovanni Maria Farinella and Antonino Furnari and Evangelos Kazakos and Jian Ma and Davide Moltisanti and Jonathan Munro and Toby Perrett and Will Price and Michael Wray},
	title = {Rescaling Egocentric Vision: Collection, Pipeline and Challenges for EPIC-KITCHENS-100},
	journal = {International Journal of Computer Vision},
	volume = {130},
	pages = {33--55},
	year = {2022},
	doi = {10.1007/s11263-021-01531-2}
} % [29]

@article{10_An_Outlook_into_the_Future_of_Egocentric_Vision,
	author = {Chiara Plizzari and Gabriele Goletto and Antonino Furnari and Siddhant Bansal and Francesco Ragusa and Giovanni Maria Farinella and Dima Damen and Tatiana Tommasi},
	title = {An Outlook into the Future of Egocentric Vision},
	journal = {International Journal of Computer Vision},
	volume = {132},
	pages = {4880--4936},
	year = {2024},
	doi = {10.1007/s11263-024-02095-7}
} % [31]

@inproceedings{3_TALL_Temporal_Activity_Localization_via_Language_Query,
	author = {Jiyang Gao and Chen Sun and Zhenheng Yang and Ram Nevatia},
	title = {TALL: Temporal Activity Localization via Language Query},
	booktitle = {arXiv preprint arXiv:1705.02101},
	year = {2017},
	url = {https://github.com/jiyanggao/TALL}
} % [52]


@misc{11_Liu2024CogVLM2_Visual_Language_Models_for_Image_and_Video_Understanding,
	author = {Zihan Liu and Jie Fu and Zihan Wang and others},
	title = {CogVLM2: Visual Language Models for Image and Video Understanding},
	year = {2024},
	note = {Cited on pp. 2, 3, 4}
}

@misc{12_Liu2024LLaVANeXT_Tackling_Multi-image_Video_and_3D_in_Large_Multimodal_Models,
	author = {Haotian Liu and Chunyuan Li and Qingyang Wu and others},
	title = {LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models},
	year = {2024},
	note = {Cited on pp. 2, 3, 4}
}

@misc{13_Wang2024InternVideo_InternVideo2.5_Empowering_Video_MLLMs_with_Long_and_Rich_Context_Modeling,
	author = {Xiaoyi Wang and Yuxuan Wang and Jianwei Yang and others},
	title = {InternVideo2.5: Empowering Video MLLMs with Long and Rich Context Modeling},
	year = {2024},
	note = {Cited on pp. 2, 3, 4}
}

@misc{14_Liu2023LLaVA_Large_Language_and_Vision_Assistant,
	author = {Haotian Liu and Chunyuan Li and Qingyang Wu and others},
	title = {LLaVA: Large Language and Vision Assistant},
	year = {2023},
	note = {Cited on p. 2}
}



