{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFNtbV7d_mBU"
   },
   "outputs": [],
   "source": [
    "# One-cell pipeline: Run LLaVA-NeXT-Video on top 50 NLQ clips and output model answers\n",
    "\n",
    "!pip install --upgrade -q accelerate bitsandbytes\n",
    "!pip install -q av\n",
    "!pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rWcPWiSM_o6W"
   },
   "outputs": [],
   "source": [
    "import json, os, zipfile\n",
    "import torch\n",
    "import numpy as np\n",
    "import av\n",
    "from google.colab import drive\n",
    "from transformers import BitsAndBytesConfig, LlavaNextVideoForConditionalGeneration, LlavaNextVideoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F6SKKlyW_qd0"
   },
   "outputs": [],
   "source": [
    "# Mount Drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcarlMvn_sAr"
   },
   "outputs": [],
   "source": [
    "json_filename = \"/content/drive/MyDrive/Colab Notebooks/Egocentric Vision/Video_Query_to_Answer/inputs/labeled_top_50_queries.json\"\n",
    "clip_folder = \"/content/drive/MyDrive/Colab Notebooks/Egocentric Vision/Video_Query_to_Answer/inputs/re_clips\"\n",
    "output_json = \"/content/drive/MyDrive/Colab Notebooks/Egocentric Vision/Video_Query_to_Answer/outputs/LLaVA_NEXT_labeled_top_50_results.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltRiC-7G_wmL"
   },
   "outputs": [],
   "source": [
    "# === LOAD MODEL ===\n",
    "quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "processor = LlavaNextVideoProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n",
    "model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
    "    \"llava-hf/LLaVA-NeXT-Video-7B-hf\", quantization_config=quant_config, device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ojHRT3ln_y30"
   },
   "outputs": [],
   "source": [
    "# === VIDEO READER ===\n",
    "def read_video_pyav(container, indices):\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start, end = indices[0], indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end: break\n",
    "        if i >= start and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([f.to_ndarray(format=\"rgb24\") for f in frames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4iNGnU9_1GI"
   },
   "outputs": [],
   "source": [
    "# === INFERENCE FUNCTION ===\n",
    "def get_model_answer(video_path, question):\n",
    "    container = av.open(video_path)\n",
    "    total_frames = container.streams.video[0].frames\n",
    "    indices = np.linspace(0, total_frames - 1, 8, dtype=int)\n",
    "    clip = read_video_pyav(container, indices)\n",
    "\n",
    "    conversation = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": question}, {\"type\": \"video\"}]}]\n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    inputs = processor([prompt], videos=[clip], padding=True, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(**inputs, max_new_tokens=100, do_sample=True, top_p=0.9)\n",
    "    return processor.batch_decode(output, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aANmJuN0_3ia"
   },
   "outputs": [],
   "source": [
    "# === PROCESS ALL CLIPS ===\n",
    "results = []\n",
    "for entry in nlq_entries:\n",
    "    clip_uid = entry[\"clip_uid\"]\n",
    "    query_idx = entry[\"query_idx\"]\n",
    "    question = entry[\"question\"]\n",
    "    gt_answer = entry[\"answer\"]\n",
    "    video_path = os.path.join(extract_dir, clip_folder, f\"{clip_uid}_{query_idx}.mp4\")\n",
    "\n",
    "    try:\n",
    "        model_answer = get_model_answer(video_path, question)\n",
    "        results.append({\n",
    "            \"clip_uid\": clip_uid,\n",
    "            \"query_idx\": query_idx,\n",
    "            \"question\": question,\n",
    "            \"gt_answer\": gt_answer,\n",
    "            \"model_answer\": model_answer\n",
    "        })\n",
    "        print(f\"‚úÖ {clip_uid}_{query_idx}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error on {clip_uid}_{query_idx}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdAQnLcv_jG8"
   },
   "outputs": [],
   "source": [
    "# === SAVE OUTPUT ===\n",
    "with open(output_json, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"üéâ Done. Results saved to: {output_json}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPVNGGDhZWjKjnt8WAiW/lz",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
